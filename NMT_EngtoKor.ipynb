{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "209b3f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "926baa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"./kor.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e007ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a471fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    w = re.sub(r\"[^a-zA-Z가-힣?.!,¿]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3664c463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence_kr(w):\n",
    "    w = w.strip()\n",
    "\n",
    "\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "    w = re.sub(r'[ |ㄱ-ㅎ|ㅏ-ㅣ]+', \" \", w)\n",
    "    w = w.strip()\n",
    "\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1898a9ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>kor</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>안녕.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>뛰어!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run.</td>\n",
       "      <td>뛰어.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who?</td>\n",
       "      <td>누구?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>우와!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3847</th>\n",
       "      <td>Science fiction has undoubtedly been the inspi...</td>\n",
       "      <td>공상 과학 소설은 의심의 여지 없이 오늘날 존재하는 많은 기술에 영감을 주었어.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3848</th>\n",
       "      <td>I started a new blog. I'll do my best not to b...</td>\n",
       "      <td>난 블로그를 시작했어. 블로그를 초반에만 반짝 많이 하다가 관두는 사람처럼은 되지 ...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3849</th>\n",
       "      <td>I think it's a shame that some foreign languag...</td>\n",
       "      <td>몇몇 외국어 선생님이 한 번도 원어민과 공부해본 적도 없으면서 대학을 나올 수 있었...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3850</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>만일 네 사정도 잘 모르는 사람이 원어민 같다고 말한다면 그건 그 사람이 네가 원어...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3851</th>\n",
       "      <td>Doubtless there exists in this world precisely...</td>\n",
       "      <td>의심의 여지 없이 세상에는 어떤 남자이든 정확히 딱 알맞는 여자와 결혼하거나 그 반...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3852 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     en  \\\n",
       "0                                                   Hi.   \n",
       "1                                                  Run!   \n",
       "2                                                  Run.   \n",
       "3                                                  Who?   \n",
       "4                                                  Wow!   \n",
       "...                                                 ...   \n",
       "3847  Science fiction has undoubtedly been the inspi...   \n",
       "3848  I started a new blog. I'll do my best not to b...   \n",
       "3849  I think it's a shame that some foreign languag...   \n",
       "3850  If someone who doesn't know your background sa...   \n",
       "3851  Doubtless there exists in this world precisely...   \n",
       "\n",
       "                                                    kor  \\\n",
       "0                                                   안녕.   \n",
       "1                                                   뛰어!   \n",
       "2                                                   뛰어.   \n",
       "3                                                   누구?   \n",
       "4                                                   우와!   \n",
       "...                                                 ...   \n",
       "3847       공상 과학 소설은 의심의 여지 없이 오늘날 존재하는 많은 기술에 영감을 주었어.   \n",
       "3848  난 블로그를 시작했어. 블로그를 초반에만 반짝 많이 하다가 관두는 사람처럼은 되지 ...   \n",
       "3849  몇몇 외국어 선생님이 한 번도 원어민과 공부해본 적도 없으면서 대학을 나올 수 있었...   \n",
       "3850  만일 네 사정도 잘 모르는 사람이 원어민 같다고 말한다면 그건 그 사람이 네가 원어...   \n",
       "3851  의심의 여지 없이 세상에는 어떤 남자이든 정확히 딱 알맞는 여자와 결혼하거나 그 반...   \n",
       "\n",
       "                                                     cc  \n",
       "0     CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
       "1     CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
       "2     CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "3     CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "4     CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
       "...                                                 ...  \n",
       "3847  CC-BY 2.0 (France) Attribution: tatoeba.org #6...  \n",
       "3848  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "3849  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
       "3850  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
       "3851  CC-BY 2.0 (France) Attribution: tatoeba.org #7...  \n",
       "\n",
       "[3852 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('kor.txt', delimiter = \"\\t\")\n",
    "data.columns = [\"en\", \"kor\", \"cc\"]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47fa07e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "    data = pd.read_csv('kor.txt', delimiter = \"\\t\")\n",
    "    data.columns = [\"en\", \"kor\", \"cc\"]\n",
    "    en = [preprocess_sentence(l) for l in data['en']]\n",
    "    kr = [preprocess_sentence_kr(l) for l in data['kor']]\n",
    "\n",
    "\n",
    "    return en, kr # kr -> eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "768fe4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "targ_lang, inp_lang = create_dataset(path_to_file, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa5c2c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ade32353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94c854a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    # creating cleaned input, output pairs\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "    print(input_tensor)\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29787223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1  532    3 ...    0    0    0]\n",
      " [   1  533   18 ...    0    0    0]\n",
      " [   1  533    3 ...    0    0    0]\n",
      " ...\n",
      " [   1  592 5801 ...    0    0    0]\n",
      " [   1 1796   43 ...    0    0    0]\n",
      " [   1 1793 1794 ...   19    3    2]]\n"
     ]
    }
   ],
   "source": [
    "num_examples = 15000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# calculate target tensor's max length \n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72231d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3081 3081 771 771\n"
     ]
    }
   ],
   "source": [
    "# valdation 20%\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad005b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5115edd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input language; index to word mapping\n",
      "1 ----> <start>\n",
      "9 ----> 나는\n",
      "994 ----> 이름을\n",
      "4401 ----> 외우는\n",
      "128 ----> 게\n",
      "1643 ----> 힘들어\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "target language; index to word mapping\n",
      "1 ----> <start>\n",
      "4 ----> i\n",
      "25 ----> have\n",
      "11 ----> a\n",
      "97 ----> hard\n",
      "68 ----> time\n",
      "1294 ----> remembering\n",
      "2093 ----> names\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"input language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"target language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ee429b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 256\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5ddf00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 97]), TensorShape([64, 112]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a54e790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b5399c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 97, 256)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 256)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69f049d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # 이를 통해 점수를 계산합니다.\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # 스코어에 self.V를 사용하기 때문에 마지막 축에 1이 들어갑니다.\n",
    "        # self.V를 사용하기 전의 텐서의 형태는 (batch_size, max_length, units)입니다.\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # 합쳐서 나온 context_vector shape == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39185090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 256)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 97, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b7422d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3dc81a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 2557)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a52ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ac1256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38a7770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2fe9f224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.5222\n",
      "Epoch 1 Loss 0.4131\n",
      "Time taken for 1 epoch 331.8143937587738 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.3547\n",
      "Epoch 2 Loss 0.3484\n",
      "Time taken for 1 epoch 338.7602758407593 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.3489\n",
      "Epoch 3 Loss 0.3259\n",
      "Time taken for 1 epoch 347.57763671875 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.3172\n",
      "Epoch 4 Loss 0.3105\n",
      "Time taken for 1 epoch 344.12548661231995 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.3204\n",
      "Epoch 5 Loss 0.2989\n",
      "Time taken for 1 epoch 346.0755217075348 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "125dfd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    sentence = preprocess_sentence_kr(sentence) \n",
    "\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # 예측된 ID는 모델에 다시 입력됩니다\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c96d6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션 가중치를 그리기 위한 함수입니다.\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10810842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8dd2fc6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x18182f5fb50>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2240998",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> 너는 눈을 왜 그렇게 <end>\n",
      "Predicted translation: i m is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 잘못된 부분 확인해보기... \n",
    "\n",
    "translate(u'너는 눈을 왜 그렇게')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
